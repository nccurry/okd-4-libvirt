# Host running Libvirt. The ansible playbooks get run on this host
libvirt_host: localhost

# Install OpenShift Container Platform or Origin Community Distribution of Kubernetes.
# Valid values are ocp or okd
install_type: okd

# Unique alphanumeric cluster name. Used to name and reference all kinds of things uniquely
cluster_name: okd

# Version of oc client to install. Used to extract installer and access the cluster
# https://mirror.openshift.com/pub/openshift-v4/clients/oc
oc_release: 4.4

# OKD release number. Used to extract installation binary when installing OKD
# Only needed when installing Origin Community Distribution of Kubernetes (i.e. install_type=okd)
# https://origin-release.svc.ci.openshift.org/
okd_release: 4.4.0-0.okd-2020-04-12-123622

# OpenShift release number. Used to extract installation binary when installing OpenShift
# Only needed when installing OpenShift Container Platform (i.e. install_type=ocp)
# https://mirror.openshift.com/pub/openshift-v4/clients/ocp/
ocp_release: 4.3.12

# Red Hat CoreOS build number. Must be of the form X.Y.Z (i.e. 4.3.8) and not latest
# Only needed when installing OpenShift Container Platform (i.e. install_type=ocp)
# http://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/
rhcos_build: 4.3.8

# Build number and stream of Fedora CoreOS
# Needed for both ocp and okd
# https://builds.coreos.fedoraproject.org/browser
fcos_stream: testing-devel
fcos_build: 31.20200407.20.2

# Location where pull secret is stored
pull_secret_path: "{{ playbook_dir + '/../pull-secret.txt' }}"

# Directory to generate ignition files
ignition_directory: "{{ lookup('env', 'HOME') }}/{{ install_type }}/{{ cluster_name }}"

# Libvirt network information
network:
  domain_suffix: lab.local
  bridge_name: virbr2
  mac: 52:54:00:1b:1c:da
  network: 10.0.10.0/24
  netmask: 255.255.255.0
  broadcast: 10.0.10.255
  gateway: 10.0.10.1
  dhcp_start: 10.0.10.10
  dhcp_end: 10.0.10.50
  upstream_dns_1: 1.1.1.1
  upstream_dns_2: 1.0.0.1

# OKD/OpenShift cluster hosts

# There should only ever be one utility server.
# This is an array to match the other cluster host data structures
utility_vms:
- name: "{{ cluster_name }}-utility.{{ network.domain_suffix }}"
  type: utility
  ip: 10.0.10.10
  mac: 52:54:00:7b:0a:15
  memory: 2048
  cpus: 1
  disk_size: 20G

# There should only ever be one bootstrap server.
# This is an array to match the other cluster host data structures
bootstrap_vms:
- name: "{{ cluster_name }}-bootstrap.{{ network.domain_suffix }}"
  type: bootstrap
  ip: 10.0.10.11
  mac: 52:54:00:dc:eb:e1
  memory: 16932
  cpus: 4
  disk_size: 20G

master_vms:
- name: "{{ cluster_name }}-master-0.{{ network.domain_suffix }}"
  type: master
  ip: 10.0.10.20
  mac: 52:54:00:4a:c3:b9
  memory: 16932
  cpus: 4
  disk_size: 20G
- name: "{{ cluster_name }}-master-1.{{ network.domain_suffix }}"
  type: master
  ip: 10.0.10.21
  mac: 52:54:00:42:3a:1d
  memory: 16932
  cpus: 4
  disk_size: 20G
- name: "{{ cluster_name }}-master-2.{{ network.domain_suffix }}"
  type: master
  ip: 10.0.10.22
  mac: 52:54:00:9d:3c:2e
  memory: 16932
  cpus: 4
  disk_size: 20G

worker_vms: []
#- name: "{{ cluster_name }}-worker-0.{{ network.domain_suffix }}"
#  type: worker
#  ip: 10.0.10.30
#  mac: 52:54:00:72:1c:71
#  memory: 16932
#  cpus: 4
#  disk_size: 20G

# Array containing all cluster vms. You shouldn't need to modify this
cluster_vms: "{{ utility_vms + bootstrap_vms + master_vms + worker_vms }}"